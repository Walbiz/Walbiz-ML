# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cX3MHOEKIXAFFrffLSIjpjAFCEDDCmSr

# EDA and Data Preperocessing

This notebook provides an exploratory data analysis on the franchise webscraping dataset. The collection process is available here in [this notebook](https://drive.google.com/uc?export=download&id=1kCsjLazcZRi8Buy87EX7cjpzySgqPyi7).

# Installing Necessary Packages
"""

pip install sentence-transformers

"""# Import Libraries"""

import tensorflow as tf
import numpy as np
import pandas as pd
import ast
import matplotlib.pyplot as plt
import seaborn as sns

from pathlib import Path
from sentence_transformers import SentenceTransformer

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

"""# Performing Basic EDA

In this section, we first load the dataset into a pandas.Dataframe and then perform some basic exploratory data analysis (EDA).
"""

franchise_data = pd.read_csv('https://drive.google.com/uc?export=download&id=1kCsjLazcZRi8Buy87EX7cjpzySgqPyi7', low_memory=False)
franchise_data.info()
franchise_data.head()

print(f"There are {len(franchise_data)} rows in the dataset.")

"""# Distribution of Categories"""

# Get all terms
all_category = [category for sublist in franchise_data['franchise_category'].tolist() for category in sublist]

# Count terms
category_count = Counter(all_category)

# Create dataframe
df_category = pd.DataFrame.from_dict(category_count, orient='index').reset_index()
df_category.columns = ['Category', 'Count']

# Sort by count and take the top 10
df_category_top10 = df_category.sort_values('Count', ascending=False).head(10)

# Plot
plt.figure(figsize=(10,6))
sns.barplot(x='Count', y='Category', data=df_category_top10, color='cornflowerblue')

# Remove top and right spines
sns.despine()

plt.title('Top 10 Category by Franchise Count')
plt.show()

"""# Word Cloud of Franchise Names"""

from wordcloud import WordCloud

# Concatenate all titles
all_names = ' '.join(franchise_data['franchise_name'].tolist())

# Create word cloud
wordcloud = WordCloud(background_color = 'white', width=800, height=400).generate(all_names)

# Plot
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Generate word cloud
wordcloud = WordCloud(background_color='white', width=800, height=400).generate(all_names)

# Get the words and their frequencies
words_frequency = wordcloud.words_

# Convert the dictionary to a list of words
word_list = list(words_frequency.keys())

# Print the list of words
print(word_list)

"""# Approaches

We will be testing two approaches to recommend papers to a user.

- The first approach is a content-based approach, where we will be recommending franchises based on the similarity of their names.
- The second approach is also a content-based approach but we will be recommending franchises based on the similarity of their description.

# Content-based approach using franchise names

Let's us start by exploring the first approach.

Since we are using the names of the franchises to recommend similar franchises, we can drop the other columns.
"""

names_dataset = franchise_data.drop(columns = ["description"])

names_dataset.head()

average_name_length = int(names_dataset['franchise_name'].apply(len).mean())
print(f"The average text length of a title is {average_name_length} characters.")

# Calculate the length of each title
names_dataset['names_length'] = names_dataset['franchise_name'].apply(len)

# Calculate the min and max length
min_length = names_dataset['names_length'].min()
max_length = names_dataset['names_length'].max()

print('The minimum length of a franchise name:', min_length)
print('The maximum length of a franchise name:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    names_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = names_lengths.mean()
    std_length = names_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(names_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(names_dataset, 'franchise_name')

plot_top_words(names_dataset, 'franchise_name')

"""# Content-based approach using descriptions

Let's us now explore the second approach.

We will be dropping the franchise_name column and keeping the description column.
"""

description_dataset = franchise_data.drop(columns = ["franchise_name"])
description_dataset.head()

average_description_length = int(description_dataset['description'].apply(len).mean())
print(f"The average text length of an description is {average_description_length} characters.")

# Calculate the length of each description
description_dataset['description_length'] = description_dataset['description'].apply(len)

# Calculate the min and max length
min_length = description_dataset['description_length'].min()
max_length = description_dataset['description_length'].max()

print('The minimum length of an description:', min_length)
print('The maximum length of an description:', max_length)

plot_length_distribution(description_dataset, 'description')

plot_top_words(description_dataset, 'description')

"""# Sentence Embeddings

This notebook contains the code to generate sentence embeddings using the pre-trained models from the [sentence-transformers](https://www.sbert.net/index.html) library.
"""

model = SentenceTransformer('all-MiniLM-L6-v2')

# Our feature we like to encode
sentences = franchise_data['franchise_name']

# Features are encoded by calling model.encode()
embeddings = model.encode(sentences)

# Print the embeddings
c = 0
for sentence, embedding in zip(sentences, embeddings):

    print("Sentence:", sentence)
    print("Embedding dimension:", len(embedding))
    print("Franchise Name length:", len(sentence))
    print("")

    if c >=5:
        break
    c +=1

import pickle

#menyimpan embeddings ke file
with open('embedding.pkl', 'wb') as file:
  pickle.dump(embeddings, file)

with open('sentences.pkl', 'wb') as file:
  pickle.dump(sentences, file)

"""# Testing the embedding model"""

franchise_you_like = input("Enter your business of interest here ðŸ‘‡ \n")
franchise_you_like

from sentence_transformers import util
cosine_scores = util.cos_sim(embeddings, model.encode(franchise_you_like))

import torch
top_similar_franchise = torch.topk(cosine_scores,dim=0, k=5,sorted=True)
top_similar_franchise

for i in top_similar_franchise.indices:
#     print(i)
    print(sentences[i.item()])